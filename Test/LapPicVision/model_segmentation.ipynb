{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Segmentation Model with Lab Dataset\n",
    "\n",
    "Based off of:\n",
    "\n",
    "https://towardsdatascience.com/train-neural-net-for-semantic-segmentation-with-pytorch-in-50-lines-of-code-830c71a6544f\n",
    "https://github.com/sagieppel/Train-Semantic-Segmentation-Net-with-Pytorch-In-50-Lines-Of-Code \n",
    "\n",
    "MIT License - use available for commercial use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "\n",
    "Learning_Rate=1e-5\n",
    "width=height=800 # image width and height\n",
    "batchSize=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainFolder=\"Data/LabPicsV1/Simple/Train/\"\n",
    "ListImages=os.listdir(os.path.join(TrainFolder, \"Image\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformImg=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)), tf.ToTensor(),tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "transformAnn=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)), tf.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadRandomImage(show=False):   \n",
    "    \n",
    "    idx=np.random.randint(0,len(ListImages)) # Pick random image   \n",
    "    Img=cv2.imread(os.path.join(TrainFolder, \"Image\",ListImages[idx]))  \n",
    "    Filled =  cv2.imread(os.path.join(TrainFolder,   \"Semantic/16_Filled\", ListImages[idx].replace(\"jpg\",\"png\")),0)       \n",
    " \n",
    "    Vessel =  cv2.imread(os.path.join(TrainFolder, \"Semantic/1_Vessel\", ListImages[idx].replace(\"jpg\",\"png\")),0) \n",
    "    AnnMap = np.zeros(Img.shape[0:2],np.float32) # Segmentation map\n",
    "\n",
    "    if show:\n",
    "        print(\"Image path: \", str(os.path.join(TrainFolder, \"Image\",ListImages[idx])))\n",
    "        # show the image, provide window name first\n",
    "        cv2.imshow('Img', Img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.imshow('Filled', Filled)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.imshow('Vessel', Vessel)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        # cv2.imshow('AnnMap', np.uint8(AnnMap))\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "        print(\"Image: \", Img)\n",
    "        print(\"Filled: \", Filled)\n",
    "        print(\"Vessel: \", Vessel)\n",
    "        print(\"AnnMap: \", AnnMap)\n",
    "    \n",
    "    if Vessel is not None:  \n",
    "        AnnMap[ Vessel == 1 ] = 1    \n",
    "    if Filled is not None:  \n",
    "        AnnMap[ Filled  == 1 ] = 2\n",
    "    \n",
    "    Img=transformImg(Img)\n",
    "    AnnMap=transformAnn(AnnMap)\n",
    "\n",
    "    return Img,AnnMap, Vessel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path:  Data/LabPicsV1/Simple/Train/Image/ChemPlayer_Nickel chloride and boride-screenshot.jpg\n",
      "Image:  [[[13 12 16]\n",
      "  [13 12 16]\n",
      "  [13 12 16]\n",
      "  ...\n",
      "  [30 25 26]\n",
      "  [30 25 26]\n",
      "  [30 25 26]]\n",
      "\n",
      " [[13 12 16]\n",
      "  [13 12 16]\n",
      "  [13 12 16]\n",
      "  ...\n",
      "  [35 30 31]\n",
      "  [35 30 31]\n",
      "  [35 30 31]]\n",
      "\n",
      " [[13 12 16]\n",
      "  [13 12 16]\n",
      "  [13 12 16]\n",
      "  ...\n",
      "  [36 31 32]\n",
      "  [36 31 32]\n",
      "  [36 31 32]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[79 79 91]\n",
      "  [74 74 86]\n",
      "  [76 76 88]\n",
      "  ...\n",
      "  [46 49 57]\n",
      "  [49 52 60]\n",
      "  [53 56 64]]\n",
      "\n",
      " [[76 76 88]\n",
      "  [67 67 79]\n",
      "  [68 68 80]\n",
      "  ...\n",
      "  [46 49 57]\n",
      "  [49 52 60]\n",
      "  [51 54 62]]\n",
      "\n",
      " [[69 69 81]\n",
      "  [68 68 80]\n",
      "  [69 69 81]\n",
      "  ...\n",
      "  [42 45 53]\n",
      "  [46 49 57]\n",
      "  [46 49 57]]]\n",
      "Filled:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Vessel:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "AnnMap:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Print test images form dataset\n",
    "Img,AnnMap, Vessel = ReadRandomImage(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Load batch of images-----------------------------------------------------\n",
    "def LoadBatch(): # Load batch of images\n",
    "    images = torch.zeros([batchSize,3,height,width])\n",
    "    ann = torch.zeros([batchSize, height, width])\n",
    "\n",
    "    for i in range(batchSize):\n",
    "        images[i],ann[i],_=ReadRandomImage()\n",
    "    \n",
    "    return images, ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carwyn/anaconda3/envs/hypertrophy/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/carwyn/anaconda3/envs/hypertrophy/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /home/carwyn/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n",
      "100%|██████████| 161M/161M [00:04<00:00, 37.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "#--------------Load and set net and optimizer-------------------------------------\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True) # Load net\n",
    "Net.classifier[4] = torch.nn.Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 3 classes\n",
    "Net=Net.to(device)\n",
    "optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) Loss= 1.0658557\n",
      "Saving Model0.torch\n",
      "1 ) Loss= 1.0500449\n",
      "2 ) Loss= 1.036717\n",
      "3 ) Loss= 0.9943116\n",
      "4 ) Loss= 1.0568877\n",
      "5 ) Loss= 1.0175107\n",
      "6 ) Loss= 1.0378245\n",
      "7 ) Loss= 1.0149018\n",
      "8 ) Loss= 1.0815524\n",
      "9 ) Loss= 0.9888705\n",
      "10 ) Loss= 1.0671489\n",
      "11 ) Loss= 0.99889594\n",
      "12 ) Loss= 1.0209575\n",
      "13 ) Loss= 0.9810101\n",
      "14 ) Loss= 0.99607784\n",
      "15 ) Loss= 0.9659469\n",
      "16 ) Loss= 0.95963675\n",
      "17 ) Loss= 0.98452586\n",
      "18 ) Loss= 0.9917831\n",
      "19 ) Loss= 0.96990806\n",
      "20 ) Loss= 0.95592886\n",
      "21 ) Loss= 1.036369\n",
      "22 ) Loss= 0.8784146\n",
      "23 ) Loss= 0.9541694\n",
      "24 ) Loss= 0.94949543\n",
      "25 ) Loss= 0.9399611\n",
      "26 ) Loss= 0.90972096\n",
      "27 ) Loss= 1.0184376\n",
      "28 ) Loss= 0.9256802\n",
      "29 ) Loss= 0.9892294\n",
      "30 ) Loss= 0.9155993\n",
      "31 ) Loss= 0.9524102\n",
      "32 ) Loss= 0.93066067\n",
      "33 ) Loss= 0.90166485\n",
      "34 ) Loss= 0.89346594\n",
      "35 ) Loss= 0.9617312\n",
      "36 ) Loss= 0.8970508\n",
      "37 ) Loss= 0.8774571\n",
      "38 ) Loss= 0.9642111\n",
      "39 ) Loss= 0.9433593\n",
      "40 ) Loss= 0.87536484\n",
      "41 ) Loss= 0.93168795\n",
      "42 ) Loss= 0.87668705\n",
      "43 ) Loss= 0.8768035\n",
      "44 ) Loss= 0.8927781\n",
      "45 ) Loss= 0.90663785\n",
      "46 ) Loss= 0.9077939\n",
      "47 ) Loss= 0.9231072\n",
      "48 ) Loss= 0.8576806\n",
      "49 ) Loss= 0.8915787\n",
      "50 ) Loss= 0.9925143\n",
      "51 ) Loss= 0.89518726\n",
      "52 ) Loss= 0.9116246\n",
      "53 ) Loss= 0.8166667\n",
      "54 ) Loss= 0.90133625\n",
      "55 ) Loss= 1.0319968\n",
      "56 ) Loss= 0.82426596\n",
      "57 ) Loss= 0.89484423\n",
      "58 ) Loss= 0.84831804\n",
      "59 ) Loss= 0.9223485\n",
      "60 ) Loss= 0.8414561\n",
      "61 ) Loss= 0.81678873\n",
      "62 ) Loss= 0.90401894\n",
      "63 ) Loss= 0.76989394\n",
      "64 ) Loss= 0.8657078\n",
      "65 ) Loss= 0.8240142\n",
      "66 ) Loss= 0.8781928\n",
      "67 ) Loss= 0.7785703\n",
      "68 ) Loss= 0.8788312\n",
      "69 ) Loss= 0.7978685\n",
      "70 ) Loss= 0.7609637\n",
      "71 ) Loss= 0.8083698\n",
      "72 ) Loss= 0.760047\n",
      "73 ) Loss= 0.86783373\n",
      "74 ) Loss= 0.7656516\n",
      "75 ) Loss= 0.82790285\n",
      "76 ) Loss= 0.75183094\n",
      "77 ) Loss= 0.89797163\n",
      "78 ) Loss= 0.839146\n",
      "79 ) Loss= 0.9295392\n",
      "80 ) Loss= 0.7007534\n",
      "81 ) Loss= 0.8013255\n",
      "82 ) Loss= 0.7709418\n",
      "83 ) Loss= 0.7666967\n",
      "84 ) Loss= 0.75927633\n",
      "85 ) Loss= 0.8411263\n",
      "86 ) Loss= 0.72829974\n",
      "87 ) Loss= 0.7689822\n",
      "88 ) Loss= 0.80513847\n",
      "89 ) Loss= 0.83694\n",
      "90 ) Loss= 0.7623721\n",
      "91 ) Loss= 0.89050406\n",
      "92 ) Loss= 0.7394984\n",
      "93 ) Loss= 0.78462946\n",
      "94 ) Loss= 0.6994882\n",
      "95 ) Loss= 0.7176756\n",
      "96 ) Loss= 0.7837033\n",
      "97 ) Loss= 0.82371324\n",
      "98 ) Loss= 0.69778717\n",
      "99 ) Loss= 0.7237171\n",
      "100 ) Loss= 0.85054016\n",
      "101 ) Loss= 0.7414569\n",
      "102 ) Loss= 0.67034966\n",
      "103 ) Loss= 0.7342032\n",
      "104 ) Loss= 0.7165005\n",
      "105 ) Loss= 0.6791565\n",
      "106 ) Loss= 0.7383428\n",
      "107 ) Loss= 0.6869063\n",
      "108 ) Loss= 0.685062\n",
      "109 ) Loss= 0.66117823\n",
      "110 ) Loss= 0.76380694\n",
      "111 ) Loss= 0.7561059\n",
      "112 ) Loss= 0.7029523\n",
      "113 ) Loss= 0.67370653\n",
      "114 ) Loss= 0.6621886\n",
      "115 ) Loss= 0.7976823\n",
      "116 ) Loss= 0.6898921\n",
      "117 ) Loss= 0.6830168\n",
      "118 ) Loss= 0.71432036\n",
      "119 ) Loss= 0.7093449\n",
      "120 ) Loss= 0.68473375\n",
      "121 ) Loss= 0.69354427\n",
      "122 ) Loss= 0.6938767\n",
      "123 ) Loss= 0.655229\n",
      "124 ) Loss= 0.7533344\n",
      "125 ) Loss= 0.72737414\n",
      "126 ) Loss= 0.82328016\n",
      "127 ) Loss= 0.6798814\n",
      "128 ) Loss= 0.690407\n",
      "129 ) Loss= 0.74730086\n",
      "130 ) Loss= 0.60203815\n",
      "131 ) Loss= 0.75246954\n",
      "132 ) Loss= 0.62754864\n",
      "133 ) Loss= 0.7175949\n",
      "134 ) Loss= 0.64802563\n",
      "135 ) Loss= 0.68849486\n",
      "136 ) Loss= 0.67616934\n",
      "137 ) Loss= 0.5973776\n",
      "138 ) Loss= 0.69134545\n",
      "139 ) Loss= 0.6016272\n",
      "140 ) Loss= 0.77383786\n",
      "141 ) Loss= 0.6428578\n",
      "142 ) Loss= 0.74162096\n",
      "143 ) Loss= 0.7188277\n",
      "144 ) Loss= 0.8441249\n",
      "145 ) Loss= 0.81017697\n",
      "146 ) Loss= 0.65512025\n",
      "147 ) Loss= 0.610657\n",
      "148 ) Loss= 0.70031744\n",
      "149 ) Loss= 0.6173494\n",
      "150 ) Loss= 0.8219455\n",
      "151 ) Loss= 0.7267848\n",
      "152 ) Loss= 0.72162825\n",
      "153 ) Loss= 0.68958324\n",
      "154 ) Loss= 0.68507814\n",
      "155 ) Loss= 0.59481543\n",
      "156 ) Loss= 0.5883281\n",
      "157 ) Loss= 0.7829626\n",
      "158 ) Loss= 0.964288\n",
      "159 ) Loss= 0.6509774\n",
      "160 ) Loss= 0.66010284\n",
      "161 ) Loss= 0.720747\n",
      "162 ) Loss= 0.7185541\n",
      "163 ) Loss= 0.6912074\n",
      "164 ) Loss= 0.6940803\n",
      "165 ) Loss= 0.8209502\n",
      "166 ) Loss= 0.6693962\n",
      "167 ) Loss= 0.6685958\n",
      "168 ) Loss= 0.65820247\n",
      "169 ) Loss= 0.5912981\n",
      "170 ) Loss= 0.63582426\n",
      "171 ) Loss= 0.66594666\n",
      "172 ) Loss= 0.6191975\n",
      "173 ) Loss= 0.6278099\n",
      "174 ) Loss= 0.75676847\n",
      "175 ) Loss= 0.6382066\n",
      "176 ) Loss= 0.65719026\n",
      "177 ) Loss= 0.6070578\n",
      "178 ) Loss= 0.58803415\n",
      "179 ) Loss= 0.6794153\n",
      "180 ) Loss= 0.5655174\n",
      "181 ) Loss= 0.75761205\n",
      "182 ) Loss= 0.6009559\n",
      "183 ) Loss= 0.6323512\n",
      "184 ) Loss= 0.55589974\n",
      "185 ) Loss= 0.90302277\n",
      "186 ) Loss= 0.68044513\n",
      "187 ) Loss= 0.56189686\n",
      "188 ) Loss= 0.55833226\n",
      "189 ) Loss= 0.60748917\n",
      "190 ) Loss= 0.52621716\n",
      "191 ) Loss= 0.59163254\n",
      "192 ) Loss= 0.5714596\n",
      "193 ) Loss= 0.60955715\n",
      "194 ) Loss= 0.6492741\n",
      "195 ) Loss= 0.6055492\n",
      "196 ) Loss= 0.6291377\n",
      "197 ) Loss= 0.66029435\n",
      "198 ) Loss= 0.5271139\n",
      "199 ) Loss= 0.6199724\n",
      "200 ) Loss= 0.6118825\n",
      "201 ) Loss= 0.5761911\n",
      "202 ) Loss= 0.54702866\n",
      "203 ) Loss= 0.7121562\n",
      "204 ) Loss= 0.7042226\n",
      "205 ) Loss= 0.49881664\n",
      "206 ) Loss= 0.5318005\n",
      "207 ) Loss= 0.6031102\n",
      "208 ) Loss= 0.591872\n",
      "209 ) Loss= 0.5489003\n",
      "210 ) Loss= 0.623427\n",
      "211 ) Loss= 0.594342\n",
      "212 ) Loss= 0.54066086\n",
      "213 ) Loss= 0.5259001\n",
      "214 ) Loss= 0.44522667\n",
      "215 ) Loss= 0.59070545\n",
      "216 ) Loss= 0.58908147\n",
      "217 ) Loss= 0.48324993\n",
      "218 ) Loss= 0.6108895\n",
      "219 ) Loss= 0.65513253\n",
      "220 ) Loss= 0.6341525\n",
      "221 ) Loss= 0.46122554\n",
      "222 ) Loss= 0.5324309\n",
      "223 ) Loss= 0.58577156\n",
      "224 ) Loss= 0.5170856\n",
      "225 ) Loss= 0.6025369\n",
      "226 ) Loss= 0.53710777\n",
      "227 ) Loss= 0.6129948\n",
      "228 ) Loss= 0.57399255\n",
      "229 ) Loss= 0.6065286\n",
      "230 ) Loss= 0.5030368\n",
      "231 ) Loss= 0.50694525\n",
      "232 ) Loss= 0.66378933\n",
      "233 ) Loss= 0.88841236\n",
      "234 ) Loss= 0.5675079\n",
      "235 ) Loss= 0.66765904\n",
      "236 ) Loss= 0.46654534\n",
      "237 ) Loss= 0.54490733\n",
      "238 ) Loss= 0.6497823\n",
      "239 ) Loss= 0.49894643\n",
      "240 ) Loss= 0.6123835\n",
      "241 ) Loss= 0.63260984\n",
      "242 ) Loss= 0.6013711\n",
      "243 ) Loss= 0.42928308\n",
      "244 ) Loss= 0.48880938\n",
      "245 ) Loss= 0.5331801\n",
      "246 ) Loss= 0.5941389\n",
      "247 ) Loss= 0.53355217\n",
      "248 ) Loss= 0.46807626\n",
      "249 ) Loss= 0.5143247\n",
      "250 ) Loss= 0.625771\n",
      "251 ) Loss= 0.52468026\n",
      "252 ) Loss= 0.44866276\n",
      "253 ) Loss= 0.4714818\n",
      "254 ) Loss= 0.51785403\n",
      "255 ) Loss= 0.569002\n",
      "256 ) Loss= 0.73250884\n",
      "257 ) Loss= 0.5066754\n",
      "258 ) Loss= 0.57760197\n",
      "259 ) Loss= 0.7122303\n",
      "260 ) Loss= 0.54250914\n",
      "261 ) Loss= 0.71412945\n",
      "262 ) Loss= 0.5213623\n",
      "263 ) Loss= 0.44916952\n",
      "264 ) Loss= 0.48455513\n",
      "265 ) Loss= 0.53658736\n",
      "266 ) Loss= 0.44868553\n",
      "267 ) Loss= 0.45479122\n",
      "268 ) Loss= 0.6516223\n",
      "269 ) Loss= 0.81378055\n",
      "270 ) Loss= 0.38677642\n",
      "271 ) Loss= 0.5029722\n",
      "272 ) Loss= 0.62308306\n",
      "273 ) Loss= 0.6666914\n",
      "274 ) Loss= 0.41118723\n",
      "275 ) Loss= 0.46918875\n",
      "276 ) Loss= 0.52668464\n",
      "277 ) Loss= 0.5117213\n",
      "278 ) Loss= 0.4445323\n",
      "279 ) Loss= 0.406369\n",
      "280 ) Loss= 0.46057752\n",
      "281 ) Loss= 0.66097224\n",
      "282 ) Loss= 0.44556826\n",
      "283 ) Loss= 0.51698446\n",
      "284 ) Loss= 0.6756507\n",
      "285 ) Loss= 0.54884857\n",
      "286 ) Loss= 0.5371873\n",
      "287 ) Loss= 0.48800653\n",
      "288 ) Loss= 0.47306198\n",
      "289 ) Loss= 0.42321163\n",
      "290 ) Loss= 0.6367864\n",
      "291 ) Loss= 0.4047683\n",
      "292 ) Loss= 0.56177896\n",
      "293 ) Loss= 0.49978906\n",
      "294 ) Loss= 0.6483077\n",
      "295 ) Loss= 0.43501055\n",
      "296 ) Loss= 0.45362642\n",
      "297 ) Loss= 0.55773985\n",
      "298 ) Loss= 0.47154865\n",
      "299 ) Loss= 0.61811805\n",
      "300 ) Loss= 0.40327835\n",
      "301 ) Loss= 0.5875047\n",
      "302 ) Loss= 0.43852887\n",
      "303 ) Loss= 0.52572525\n",
      "304 ) Loss= 0.524735\n",
      "305 ) Loss= 0.5671368\n",
      "306 ) Loss= 0.5335846\n",
      "307 ) Loss= 0.505773\n",
      "308 ) Loss= 0.47664133\n",
      "309 ) Loss= 0.4123422\n",
      "310 ) Loss= 0.54953575\n",
      "311 ) Loss= 0.56057805\n",
      "312 ) Loss= 0.55630076\n",
      "313 ) Loss= 0.48709702\n",
      "314 ) Loss= 0.52650875\n",
      "315 ) Loss= 0.42568433\n",
      "316 ) Loss= 0.4531276\n",
      "317 ) Loss= 0.5087028\n",
      "318 ) Loss= 0.49263\n",
      "319 ) Loss= 0.39482105\n",
      "320 ) Loss= 0.5700169\n",
      "321 ) Loss= 0.47738156\n",
      "322 ) Loss= 0.36853242\n",
      "323 ) Loss= 0.90282094\n",
      "324 ) Loss= 0.55838954\n",
      "325 ) Loss= 0.6344714\n",
      "326 ) Loss= 0.4129173\n",
      "327 ) Loss= 0.4249932\n",
      "328 ) Loss= 0.504805\n",
      "329 ) Loss= 0.7622756\n",
      "330 ) Loss= 0.6571374\n",
      "331 ) Loss= 0.46249804\n",
      "332 ) Loss= 0.5894148\n",
      "333 ) Loss= 0.48974422\n",
      "334 ) Loss= 0.53244334\n",
      "335 ) Loss= 0.46288487\n",
      "336 ) Loss= 0.47525075\n",
      "337 ) Loss= 0.43968022\n",
      "338 ) Loss= 0.6297742\n",
      "339 ) Loss= 0.40366542\n",
      "340 ) Loss= 0.41238245\n",
      "341 ) Loss= 0.5558614\n",
      "342 ) Loss= 0.38940123\n",
      "343 ) Loss= 0.54950625\n",
      "344 ) Loss= 0.3880641\n",
      "345 ) Loss= 0.46676376\n",
      "346 ) Loss= 0.3798333\n",
      "347 ) Loss= 0.43415362\n",
      "348 ) Loss= 0.47830418\n",
      "349 ) Loss= 0.3241401\n",
      "350 ) Loss= 0.42805287\n",
      "351 ) Loss= 0.5467982\n",
      "352 ) Loss= 0.46748006\n",
      "353 ) Loss= 0.4667237\n",
      "354 ) Loss= 0.66845584\n",
      "355 ) Loss= 0.42640224\n",
      "356 ) Loss= 0.41465843\n",
      "357 ) Loss= 0.5673882\n",
      "358 ) Loss= 0.58385366\n",
      "359 ) Loss= 0.5650119\n",
      "360 ) Loss= 0.33378258\n",
      "361 ) Loss= 0.3629943\n",
      "362 ) Loss= 0.5007091\n",
      "363 ) Loss= 0.49332356\n",
      "364 ) Loss= 0.5490887\n",
      "365 ) Loss= 0.6354373\n",
      "366 ) Loss= 0.44166145\n",
      "367 ) Loss= 0.44567734\n",
      "368 ) Loss= 0.4472007\n",
      "369 ) Loss= 0.5634193\n",
      "370 ) Loss= 0.5596108\n",
      "371 ) Loss= 0.38482878\n",
      "372 ) Loss= 0.6309944\n",
      "373 ) Loss= 0.34105557\n",
      "374 ) Loss= 0.5293916\n",
      "375 ) Loss= 0.43505776\n",
      "376 ) Loss= 0.47922492\n",
      "377 ) Loss= 0.37739256\n",
      "378 ) Loss= 0.40105143\n",
      "379 ) Loss= 0.4732504\n",
      "380 ) Loss= 0.38640144\n",
      "381 ) Loss= 0.42798907\n",
      "382 ) Loss= 0.3902617\n",
      "383 ) Loss= 0.36118\n",
      "384 ) Loss= 0.36947942\n",
      "385 ) Loss= 0.509956\n",
      "386 ) Loss= 0.41815785\n",
      "387 ) Loss= 0.34300587\n",
      "388 ) Loss= 0.4737417\n",
      "389 ) Loss= 0.61677414\n",
      "390 ) Loss= 0.3658181\n",
      "391 ) Loss= 0.6191539\n",
      "392 ) Loss= 0.46241572\n",
      "393 ) Loss= 0.33767217\n",
      "394 ) Loss= 0.38190162\n",
      "395 ) Loss= 0.49287623\n",
      "396 ) Loss= 0.28099284\n",
      "397 ) Loss= 0.3696603\n",
      "398 ) Loss= 0.39090765\n",
      "399 ) Loss= 0.37606183\n",
      "400 ) Loss= 0.36348915\n",
      "401 ) Loss= 0.47282967\n",
      "402 ) Loss= 0.5585513\n",
      "403 ) Loss= 0.40315303\n",
      "404 ) Loss= 0.338285\n",
      "405 ) Loss= 0.51105857\n",
      "406 ) Loss= 0.4746387\n",
      "407 ) Loss= 0.35112748\n",
      "408 ) Loss= 0.3817487\n",
      "409 ) Loss= 0.38783514\n",
      "410 ) Loss= 0.5215809\n",
      "411 ) Loss= 0.4701174\n",
      "412 ) Loss= 0.6235053\n",
      "413 ) Loss= 0.5440021\n",
      "414 ) Loss= 0.47667715\n",
      "415 ) Loss= 0.46599564\n",
      "416 ) Loss= 0.32734233\n",
      "417 ) Loss= 0.37525544\n",
      "418 ) Loss= 0.50347227\n",
      "419 ) Loss= 0.3584465\n",
      "420 ) Loss= 0.36673868\n",
      "421 ) Loss= 0.4744446\n",
      "422 ) Loss= 0.33062318\n",
      "423 ) Loss= 0.42505318\n",
      "424 ) Loss= 0.509004\n",
      "425 ) Loss= 0.5406155\n",
      "426 ) Loss= 0.37737364\n",
      "427 ) Loss= 0.3615905\n",
      "428 ) Loss= 0.363303\n",
      "429 ) Loss= 0.5531034\n",
      "430 ) Loss= 0.39819974\n",
      "431 ) Loss= 0.53510517\n",
      "432 ) Loss= 0.49417564\n",
      "433 ) Loss= 0.43856323\n",
      "434 ) Loss= 0.8493758\n",
      "435 ) Loss= 0.4138245\n",
      "436 ) Loss= 0.47827098\n",
      "437 ) Loss= 0.43728936\n",
      "438 ) Loss= 0.40520427\n",
      "439 ) Loss= 0.45946956\n",
      "440 ) Loss= 0.37051412\n",
      "441 ) Loss= 0.38305494\n",
      "442 ) Loss= 0.35595286\n",
      "443 ) Loss= 0.54475844\n",
      "444 ) Loss= 0.47337678\n",
      "445 ) Loss= 0.46300024\n",
      "446 ) Loss= 0.4585293\n",
      "447 ) Loss= 0.40611637\n",
      "448 ) Loss= 0.3634513\n",
      "449 ) Loss= 0.48472703\n",
      "450 ) Loss= 0.39536604\n",
      "451 ) Loss= 0.3506818\n",
      "452 ) Loss= 0.47501966\n",
      "453 ) Loss= 0.41205046\n",
      "454 ) Loss= 0.52515954\n",
      "455 ) Loss= 0.38639835\n",
      "456 ) Loss= 0.30832636\n",
      "457 ) Loss= 0.36079502\n",
      "458 ) Loss= 0.46821803\n",
      "459 ) Loss= 0.5175271\n",
      "460 ) Loss= 0.4266168\n",
      "461 ) Loss= 0.44662744\n",
      "462 ) Loss= 0.5464822\n",
      "463 ) Loss= 0.43280676\n",
      "464 ) Loss= 0.30267084\n",
      "465 ) Loss= 0.6553525\n",
      "466 ) Loss= 0.70074266\n",
      "467 ) Loss= 0.4824819\n",
      "468 ) Loss= 0.48134413\n",
      "469 ) Loss= 0.33864915\n",
      "470 ) Loss= 0.46347415\n",
      "471 ) Loss= 0.4959936\n",
      "472 ) Loss= 0.360878\n",
      "473 ) Loss= 0.5285666\n",
      "474 ) Loss= 0.5140568\n",
      "475 ) Loss= 0.32753408\n",
      "476 ) Loss= 0.4116725\n",
      "477 ) Loss= 0.36147466\n",
      "478 ) Loss= 0.40905043\n",
      "479 ) Loss= 0.35992533\n",
      "480 ) Loss= 0.37878886\n",
      "481 ) Loss= 0.32883385\n",
      "482 ) Loss= 0.41771692\n",
      "483 ) Loss= 0.39396223\n",
      "484 ) Loss= 0.34409603\n",
      "485 ) Loss= 0.37846127\n",
      "486 ) Loss= 0.36512682\n",
      "487 ) Loss= 0.6057876\n",
      "488 ) Loss= 0.42251638\n",
      "489 ) Loss= 0.81258464\n",
      "490 ) Loss= 0.67934716\n",
      "491 ) Loss= 0.46510562\n",
      "492 ) Loss= 0.64742535\n",
      "493 ) Loss= 0.39195758\n",
      "494 ) Loss= 0.38494924\n",
      "495 ) Loss= 0.5380336\n",
      "496 ) Loss= 0.64759153\n",
      "497 ) Loss= 0.49361432\n",
      "498 ) Loss= 0.40887806\n",
      "499 ) Loss= 0.46958292\n",
      "500 ) Loss= 0.3647871\n",
      "501 ) Loss= 0.36814857\n",
      "502 ) Loss= 0.35243255\n",
      "503 ) Loss= 0.41698843\n",
      "504 ) Loss= 0.4871261\n",
      "505 ) Loss= 0.29141575\n",
      "506 ) Loss= 0.41439515\n",
      "507 ) Loss= 0.5832215\n",
      "508 ) Loss= 0.48712042\n",
      "509 ) Loss= 0.41214058\n",
      "510 ) Loss= 0.43438825\n",
      "511 ) Loss= 0.30290854\n",
      "512 ) Loss= 0.49140278\n",
      "513 ) Loss= 0.53841865\n",
      "514 ) Loss= 0.3434855\n",
      "515 ) Loss= 0.30998003\n",
      "516 ) Loss= 0.3646658\n",
      "517 ) Loss= 0.64067346\n",
      "518 ) Loss= 0.30089554\n",
      "519 ) Loss= 0.46422043\n",
      "520 ) Loss= 0.43340817\n",
      "521 ) Loss= 0.661674\n",
      "522 ) Loss= 0.45947644\n",
      "523 ) Loss= 0.2867471\n",
      "524 ) Loss= 0.32504773\n",
      "525 ) Loss= 0.3557327\n",
      "526 ) Loss= 0.42457452\n",
      "527 ) Loss= 0.59077924\n",
      "528 ) Loss= 0.329414\n",
      "529 ) Loss= 0.3691713\n",
      "530 ) Loss= 0.40449283\n",
      "531 ) Loss= 0.2974375\n",
      "532 ) Loss= 0.34222576\n",
      "533 ) Loss= 0.34219098\n",
      "534 ) Loss= 0.31083792\n",
      "535 ) Loss= 0.45565927\n",
      "536 ) Loss= 0.33873868\n",
      "537 ) Loss= 0.3520349\n",
      "538 ) Loss= 0.28812355\n",
      "539 ) Loss= 0.5253842\n",
      "540 ) Loss= 0.47173646\n",
      "541 ) Loss= 0.5052066\n",
      "542 ) Loss= 0.4213463\n",
      "543 ) Loss= 0.5620315\n",
      "544 ) Loss= 0.4264772\n",
      "545 ) Loss= 0.2918811\n",
      "546 ) Loss= 0.3485096\n",
      "547 ) Loss= 0.2599586\n",
      "548 ) Loss= 0.47766984\n",
      "549 ) Loss= 0.699038\n",
      "550 ) Loss= 0.465212\n",
      "551 ) Loss= 0.39707395\n",
      "552 ) Loss= 0.31159657\n",
      "553 ) Loss= 0.5657863\n",
      "554 ) Loss= 0.37280965\n",
      "555 ) Loss= 0.32780102\n",
      "556 ) Loss= 0.3599202\n",
      "557 ) Loss= 0.40184394\n",
      "558 ) Loss= 0.44850275\n",
      "559 ) Loss= 0.34205046\n",
      "560 ) Loss= 0.3161343\n",
      "561 ) Loss= 0.56988555\n",
      "562 ) Loss= 0.58938\n",
      "563 ) Loss= 0.37333554\n",
      "564 ) Loss= 0.4191866\n",
      "565 ) Loss= 0.3454136\n",
      "566 ) Loss= 0.36280915\n",
      "567 ) Loss= 0.4513636\n",
      "568 ) Loss= 0.3850382\n",
      "569 ) Loss= 0.3522539\n",
      "570 ) Loss= 0.44309643\n",
      "571 ) Loss= 0.43347156\n",
      "572 ) Loss= 0.3641725\n",
      "573 ) Loss= 0.29849005\n",
      "574 ) Loss= 0.42263198\n",
      "575 ) Loss= 0.73481417\n",
      "576 ) Loss= 0.5178375\n",
      "577 ) Loss= 0.37786385\n",
      "578 ) Loss= 0.5676121\n",
      "579 ) Loss= 0.3225901\n",
      "580 ) Loss= 0.44413757\n",
      "581 ) Loss= 0.3177395\n",
      "582 ) Loss= 0.5536634\n",
      "583 ) Loss= 0.4325069\n",
      "584 ) Loss= 0.42497763\n",
      "585 ) Loss= 0.4587095\n",
      "586 ) Loss= 0.31642467\n",
      "587 ) Loss= 0.40902996\n",
      "588 ) Loss= 0.36062056\n",
      "589 ) Loss= 0.3022828\n",
      "590 ) Loss= 0.5867896\n",
      "591 ) Loss= 0.4239822\n",
      "592 ) Loss= 0.4785156\n",
      "593 ) Loss= 0.4304269\n",
      "594 ) Loss= 0.41824484\n",
      "595 ) Loss= 0.3688384\n",
      "596 ) Loss= 0.40309104\n",
      "597 ) Loss= 0.36333513\n",
      "598 ) Loss= 0.2976574\n",
      "599 ) Loss= 0.43897474\n",
      "600 ) Loss= 0.33666876\n",
      "601 ) Loss= 0.5437574\n",
      "602 ) Loss= 0.3390055\n",
      "603 ) Loss= 0.310084\n",
      "604 ) Loss= 0.29409537\n",
      "605 ) Loss= 0.31197724\n",
      "606 ) Loss= 0.36831924\n",
      "607 ) Loss= 0.34438866\n",
      "608 ) Loss= 0.6832275\n",
      "609 ) Loss= 0.47584596\n",
      "610 ) Loss= 0.4148012\n",
      "611 ) Loss= 0.28732982\n",
      "612 ) Loss= 0.32873628\n",
      "613 ) Loss= 0.4123952\n",
      "614 ) Loss= 0.36268887\n",
      "615 ) Loss= 0.34640586\n",
      "616 ) Loss= 0.34008437\n",
      "617 ) Loss= 0.36494473\n",
      "618 ) Loss= 0.4060634\n",
      "619 ) Loss= 0.39064738\n",
      "620 ) Loss= 0.4188495\n",
      "621 ) Loss= 0.35056743\n",
      "622 ) Loss= 0.33736897\n",
      "623 ) Loss= 0.25637236\n",
      "624 ) Loss= 0.341711\n",
      "625 ) Loss= 0.29746956\n",
      "626 ) Loss= 0.292509\n",
      "627 ) Loss= 0.3137823\n",
      "628 ) Loss= 0.3325732\n",
      "629 ) Loss= 0.29382062\n",
      "630 ) Loss= 0.3270478\n",
      "631 ) Loss= 0.45480683\n",
      "632 ) Loss= 0.51290786\n",
      "633 ) Loss= 0.32204467\n",
      "634 ) Loss= 0.65281326\n",
      "635 ) Loss= 0.27509356\n",
      "636 ) Loss= 0.41932407\n",
      "637 ) Loss= 0.28306758\n",
      "638 ) Loss= 0.40420377\n",
      "639 ) Loss= 0.6263185\n",
      "640 ) Loss= 0.39685372\n",
      "641 ) Loss= 0.6872764\n",
      "642 ) Loss= 0.4515427\n",
      "643 ) Loss= 0.28931895\n",
      "644 ) Loss= 0.28314057\n",
      "645 ) Loss= 0.31703058\n",
      "646 ) Loss= 0.5071771\n",
      "647 ) Loss= 0.5433195\n",
      "648 ) Loss= 0.56787103\n",
      "649 ) Loss= 0.3242653\n",
      "650 ) Loss= 0.3768627\n",
      "651 ) Loss= 0.37618142\n",
      "652 ) Loss= 0.36301643\n",
      "653 ) Loss= 0.45110074\n",
      "654 ) Loss= 0.33255625\n",
      "655 ) Loss= 0.26602238\n",
      "656 ) Loss= 0.34681708\n",
      "657 ) Loss= 0.35934296\n",
      "658 ) Loss= 0.28779885\n",
      "659 ) Loss= 0.31885165\n",
      "660 ) Loss= 0.31729308\n",
      "661 ) Loss= 0.4222401\n",
      "662 ) Loss= 0.43060464\n",
      "663 ) Loss= 0.40133968\n",
      "664 ) Loss= 0.4135802\n",
      "665 ) Loss= 0.35422966\n",
      "666 ) Loss= 0.40431353\n",
      "667 ) Loss= 0.33516088\n",
      "668 ) Loss= 0.4081967\n",
      "669 ) Loss= 0.28355855\n",
      "670 ) Loss= 0.3282358\n",
      "671 ) Loss= 0.2628385\n",
      "672 ) Loss= 0.39503378\n",
      "673 ) Loss= 0.36925268\n",
      "674 ) Loss= 0.33752316\n",
      "675 ) Loss= 0.36994478\n",
      "676 ) Loss= 0.3376169\n",
      "677 ) Loss= 0.36944297\n",
      "678 ) Loss= 0.41630143\n",
      "679 ) Loss= 0.88666445\n",
      "680 ) Loss= 0.32250202\n",
      "681 ) Loss= 0.34302884\n",
      "682 ) Loss= 0.2871175\n",
      "683 ) Loss= 0.4243065\n",
      "684 ) Loss= 0.32960212\n",
      "685 ) Loss= 0.55373496\n",
      "686 ) Loss= 0.38959113\n",
      "687 ) Loss= 0.41794813\n",
      "688 ) Loss= 0.5261369\n",
      "689 ) Loss= 0.42509833\n",
      "690 ) Loss= 0.34702465\n",
      "691 ) Loss= 0.29446816\n",
      "692 ) Loss= 0.46211395\n",
      "693 ) Loss= 0.33743483\n",
      "694 ) Loss= 0.2892896\n",
      "695 ) Loss= 0.3218263\n",
      "696 ) Loss= 0.39823407\n",
      "697 ) Loss= 0.4797536\n",
      "698 ) Loss= 0.4087466\n",
      "699 ) Loss= 0.48388326\n",
      "700 ) Loss= 0.36130252\n",
      "701 ) Loss= 0.429127\n",
      "702 ) Loss= 0.29925945\n",
      "703 ) Loss= 0.2789739\n",
      "704 ) Loss= 0.3100936\n",
      "705 ) Loss= 0.34278926\n",
      "706 ) Loss= 0.51843387\n",
      "707 ) Loss= 0.45479974\n",
      "708 ) Loss= 0.41636422\n",
      "709 ) Loss= 0.3112354\n",
      "710 ) Loss= 0.34814924\n",
      "711 ) Loss= 0.35552722\n",
      "712 ) Loss= 0.296003\n",
      "713 ) Loss= 0.36839864\n",
      "714 ) Loss= 0.34232807\n",
      "715 ) Loss= 0.3025606\n",
      "716 ) Loss= 0.45877102\n",
      "717 ) Loss= 0.51917523\n",
      "718 ) Loss= 0.25286683\n",
      "719 ) Loss= 0.33664992\n",
      "720 ) Loss= 0.41693527\n",
      "721 ) Loss= 0.30065534\n",
      "722 ) Loss= 0.29408514\n",
      "723 ) Loss= 0.25522447\n",
      "724 ) Loss= 0.40597025\n",
      "725 ) Loss= 0.53063613\n",
      "726 ) Loss= 0.29154763\n",
      "727 ) Loss= 0.24576534\n",
      "728 ) Loss= 0.24653105\n",
      "729 ) Loss= 0.59201896\n",
      "730 ) Loss= 0.39110306\n",
      "731 ) Loss= 0.7397526\n",
      "732 ) Loss= 0.33123672\n",
      "733 ) Loss= 0.40771702\n",
      "734 ) Loss= 0.26978964\n",
      "735 ) Loss= 0.34474546\n",
      "736 ) Loss= 0.34883177\n",
      "737 ) Loss= 0.2947015\n",
      "738 ) Loss= 0.7541757\n",
      "739 ) Loss= 0.42390382\n",
      "740 ) Loss= 0.5498732\n",
      "741 ) Loss= 0.32057056\n",
      "742 ) Loss= 0.4227352\n",
      "743 ) Loss= 0.34937686\n",
      "744 ) Loss= 0.3347355\n",
      "745 ) Loss= 0.33016798\n",
      "746 ) Loss= 0.59912544\n",
      "747 ) Loss= 0.39482912\n",
      "748 ) Loss= 0.49249646\n",
      "749 ) Loss= 0.41472927\n",
      "750 ) Loss= 0.22949915\n",
      "751 ) Loss= 0.3139799\n",
      "752 ) Loss= 0.2546749\n",
      "753 ) Loss= 0.3939999\n",
      "754 ) Loss= 0.39508587\n",
      "755 ) Loss= 0.5188403\n",
      "756 ) Loss= 0.27648875\n",
      "757 ) Loss= 0.34167367\n",
      "758 ) Loss= 0.63622576\n",
      "759 ) Loss= 0.43798244\n",
      "760 ) Loss= 0.6019288\n",
      "761 ) Loss= 0.23670709\n",
      "762 ) Loss= 0.3124472\n",
      "763 ) Loss= 0.7404569\n",
      "764 ) Loss= 0.5577694\n",
      "765 ) Loss= 0.25228453\n",
      "766 ) Loss= 0.33749032\n",
      "767 ) Loss= 0.2563433\n",
      "768 ) Loss= 0.38884383\n",
      "769 ) Loss= 0.37084106\n",
      "770 ) Loss= 0.38844466\n",
      "771 ) Loss= 0.6511066\n",
      "772 ) Loss= 0.51309985\n",
      "773 ) Loss= 0.30116567\n",
      "774 ) Loss= 0.36673018\n",
      "775 ) Loss= 0.47268865\n",
      "776 ) Loss= 0.3046888\n",
      "777 ) Loss= 0.49151218\n",
      "778 ) Loss= 0.23699467\n",
      "779 ) Loss= 0.2589357\n",
      "780 ) Loss= 0.2435253\n",
      "781 ) Loss= 0.36266112\n",
      "782 ) Loss= 0.53940994\n",
      "783 ) Loss= 0.2487507\n",
      "784 ) Loss= 0.35913163\n",
      "785 ) Loss= 0.24725506\n",
      "786 ) Loss= 0.3471714\n",
      "787 ) Loss= 0.38301417\n",
      "788 ) Loss= 0.3008244\n",
      "789 ) Loss= 0.22867939\n",
      "790 ) Loss= 0.2530451\n",
      "791 ) Loss= 0.41654205\n",
      "792 ) Loss= 0.28785756\n",
      "793 ) Loss= 0.33041888\n",
      "794 ) Loss= 0.5815972\n",
      "795 ) Loss= 0.3214656\n",
      "796 ) Loss= 0.2832768\n",
      "797 ) Loss= 0.2618197\n",
      "798 ) Loss= 0.3533924\n",
      "799 ) Loss= 0.49199733\n",
      "800 ) Loss= 0.54868025\n",
      "801 ) Loss= 0.27958834\n",
      "802 ) Loss= 0.26783594\n",
      "803 ) Loss= 0.24962318\n",
      "804 ) Loss= 0.43547514\n",
      "805 ) Loss= 0.47381717\n",
      "806 ) Loss= 0.5899894\n",
      "807 ) Loss= 0.41493392\n",
      "808 ) Loss= 0.29620832\n",
      "809 ) Loss= 0.36949262\n",
      "810 ) Loss= 0.39698875\n",
      "811 ) Loss= 0.24670884\n",
      "812 ) Loss= 0.6586871\n",
      "813 ) Loss= 0.23128545\n",
      "814 ) Loss= 0.4967797\n",
      "815 ) Loss= 0.34223256\n",
      "816 ) Loss= 0.439865\n",
      "817 ) Loss= 0.32968533\n",
      "818 ) Loss= 0.34225723\n",
      "819 ) Loss= 0.24807033\n",
      "820 ) Loss= 0.30541506\n",
      "821 ) Loss= 0.4919539\n",
      "822 ) Loss= 0.26674023\n",
      "823 ) Loss= 0.2709348\n",
      "824 ) Loss= 0.34254223\n",
      "825 ) Loss= 0.35291362\n",
      "826 ) Loss= 0.37630624\n",
      "827 ) Loss= 0.4724764\n",
      "828 ) Loss= 0.32238302\n",
      "829 ) Loss= 0.31647462\n",
      "830 ) Loss= 0.3330919\n",
      "831 ) Loss= 0.5152005\n",
      "832 ) Loss= 0.4038354\n",
      "833 ) Loss= 0.33767018\n",
      "834 ) Loss= 0.67467755\n",
      "835 ) Loss= 0.27367514\n",
      "836 ) Loss= 0.4909623\n",
      "837 ) Loss= 0.34524763\n",
      "838 ) Loss= 0.4362086\n",
      "839 ) Loss= 0.9304995\n",
      "840 ) Loss= 0.73304915\n",
      "841 ) Loss= 0.3295881\n",
      "842 ) Loss= 0.22294915\n",
      "843 ) Loss= 0.41670093\n",
      "844 ) Loss= 0.5172449\n",
      "845 ) Loss= 0.37811175\n",
      "846 ) Loss= 0.36103997\n",
      "847 ) Loss= 0.4541457\n",
      "848 ) Loss= 0.454715\n",
      "849 ) Loss= 0.36184973\n",
      "850 ) Loss= 0.33394784\n",
      "851 ) Loss= 0.5232548\n",
      "852 ) Loss= 0.27417138\n",
      "853 ) Loss= 0.40762538\n",
      "854 ) Loss= 0.2897211\n",
      "855 ) Loss= 0.32184187\n",
      "856 ) Loss= 0.31166074\n",
      "857 ) Loss= 0.3756801\n",
      "858 ) Loss= 0.38431126\n",
      "859 ) Loss= 0.48030156\n",
      "860 ) Loss= 0.4927716\n",
      "861 ) Loss= 0.25317407\n",
      "862 ) Loss= 0.46041748\n",
      "863 ) Loss= 0.29144773\n",
      "864 ) Loss= 0.42197272\n",
      "865 ) Loss= 0.2602353\n",
      "866 ) Loss= 0.27616346\n",
      "867 ) Loss= 0.39225227\n",
      "868 ) Loss= 0.3160422\n",
      "869 ) Loss= 0.41482693\n",
      "870 ) Loss= 0.405686\n",
      "871 ) Loss= 0.47682688\n",
      "872 ) Loss= 0.2563984\n",
      "873 ) Loss= 0.25791642\n",
      "874 ) Loss= 0.31558853\n",
      "875 ) Loss= 0.5701997\n",
      "876 ) Loss= 0.36443773\n",
      "877 ) Loss= 0.3818607\n",
      "878 ) Loss= 0.4660171\n",
      "879 ) Loss= 0.41039222\n",
      "880 ) Loss= 0.6541034\n",
      "881 ) Loss= 0.52094066\n",
      "882 ) Loss= 0.38232297\n",
      "883 ) Loss= 0.25717002\n",
      "884 ) Loss= 0.25292268\n",
      "885 ) Loss= 0.3924629\n",
      "886 ) Loss= 0.42677754\n",
      "887 ) Loss= 0.48042607\n",
      "888 ) Loss= 0.21027642\n",
      "889 ) Loss= 0.42753848\n",
      "890 ) Loss= 0.26055327\n",
      "891 ) Loss= 0.23435362\n",
      "892 ) Loss= 0.29579058\n",
      "893 ) Loss= 0.340109\n",
      "894 ) Loss= 0.26303756\n",
      "895 ) Loss= 0.20276037\n",
      "896 ) Loss= 0.3418243\n",
      "897 ) Loss= 0.3303442\n",
      "898 ) Loss= 0.2964455\n",
      "899 ) Loss= 0.40937415\n",
      "900 ) Loss= 0.2726324\n",
      "901 ) Loss= 0.38368818\n",
      "902 ) Loss= 0.50046724\n",
      "903 ) Loss= 0.32477906\n",
      "904 ) Loss= 0.4212369\n",
      "905 ) Loss= 0.3292429\n",
      "906 ) Loss= 0.28001863\n",
      "907 ) Loss= 0.5922984\n",
      "908 ) Loss= 0.2888784\n",
      "909 ) Loss= 0.32747656\n",
      "910 ) Loss= 0.2578277\n",
      "911 ) Loss= 0.2649768\n",
      "912 ) Loss= 0.51472366\n",
      "913 ) Loss= 0.3144079\n",
      "914 ) Loss= 0.3329333\n",
      "915 ) Loss= 0.2604138\n",
      "916 ) Loss= 0.42714423\n",
      "917 ) Loss= 0.49390918\n",
      "918 ) Loss= 0.3314166\n",
      "919 ) Loss= 0.305121\n",
      "920 ) Loss= 0.35257536\n",
      "921 ) Loss= 0.38898686\n",
      "922 ) Loss= 0.3335248\n",
      "923 ) Loss= 0.26991653\n",
      "924 ) Loss= 0.24697286\n",
      "925 ) Loss= 0.3439094\n",
      "926 ) Loss= 0.3234282\n",
      "927 ) Loss= 0.32277158\n",
      "928 ) Loss= 0.27656728\n",
      "929 ) Loss= 0.392291\n",
      "930 ) Loss= 0.31987265\n",
      "931 ) Loss= 0.3171923\n",
      "932 ) Loss= 0.38058805\n",
      "933 ) Loss= 0.53695726\n",
      "934 ) Loss= 0.48262656\n",
      "935 ) Loss= 0.36686373\n",
      "936 ) Loss= 0.5195178\n",
      "937 ) Loss= 0.40563825\n",
      "938 ) Loss= 0.39552113\n",
      "939 ) Loss= 0.31736407\n",
      "940 ) Loss= 0.34752175\n",
      "941 ) Loss= 0.5007968\n",
      "942 ) Loss= 0.3532955\n",
      "943 ) Loss= 0.32312438\n",
      "944 ) Loss= 0.27167425\n",
      "945 ) Loss= 0.40982378\n",
      "946 ) Loss= 0.4744515\n",
      "947 ) Loss= 0.3714642\n",
      "948 ) Loss= 0.27073002\n",
      "949 ) Loss= 0.37566754\n",
      "950 ) Loss= 0.25852215\n",
      "951 ) Loss= 0.4269368\n",
      "952 ) Loss= 0.24788663\n",
      "953 ) Loss= 0.28702703\n",
      "954 ) Loss= 0.2362461\n",
      "955 ) Loss= 0.36532792\n",
      "956 ) Loss= 0.36803165\n",
      "957 ) Loss= 0.49746594\n",
      "958 ) Loss= 0.3348338\n",
      "959 ) Loss= 0.33055225\n",
      "960 ) Loss= 0.1983525\n",
      "961 ) Loss= 0.45014545\n",
      "962 ) Loss= 0.60012096\n",
      "963 ) Loss= 0.28759223\n",
      "964 ) Loss= 0.2241848\n",
      "965 ) Loss= 0.32602417\n",
      "966 ) Loss= 0.31441122\n",
      "967 ) Loss= 0.22066802\n",
      "968 ) Loss= 0.4791123\n",
      "969 ) Loss= 0.3307894\n",
      "970 ) Loss= 0.25933158\n",
      "971 ) Loss= 0.28209245\n",
      "972 ) Loss= 0.37599373\n",
      "973 ) Loss= 0.29729393\n",
      "974 ) Loss= 0.38774887\n",
      "975 ) Loss= 0.39794862\n",
      "976 ) Loss= 0.30951664\n",
      "977 ) Loss= 0.30586436\n",
      "978 ) Loss= 0.27796018\n",
      "979 ) Loss= 0.23014437\n",
      "980 ) Loss= 0.3609905\n",
      "981 ) Loss= 0.27164096\n",
      "982 ) Loss= 0.40406987\n",
      "983 ) Loss= 0.22844055\n",
      "984 ) Loss= 0.3657424\n",
      "985 ) Loss= 0.20373398\n",
      "986 ) Loss= 0.3120593\n",
      "987 ) Loss= 0.30571017\n",
      "988 ) Loss= 0.21972954\n",
      "989 ) Loss= 0.33860362\n",
      "990 ) Loss= 0.2631159\n",
      "991 ) Loss= 0.29466176\n",
      "992 ) Loss= 0.2554409\n",
      "993 ) Loss= 0.3942982\n",
      "994 ) Loss= 0.32851723\n",
      "995 ) Loss= 0.27781832\n",
      "996 ) Loss= 0.5624688\n",
      "997 ) Loss= 0.3218834\n",
      "998 ) Loss= 0.47525048\n",
      "999 ) Loss= 0.37179378\n",
      "1000 ) Loss= 0.3278686\n",
      "Saving Model1000.torch\n",
      "1001 ) Loss= 0.20080861\n",
      "1002 ) Loss= 0.34425443\n",
      "1003 ) Loss= 0.74983543\n",
      "1004 ) Loss= 0.31372464\n",
      "1005 ) Loss= 0.2652109\n",
      "1006 ) Loss= 0.29247636\n",
      "1007 ) Loss= 0.36104837\n",
      "1008 ) Loss= 0.3572162\n",
      "1009 ) Loss= 0.31756088\n",
      "1010 ) Loss= 0.28610283\n",
      "1011 ) Loss= 0.2326389\n",
      "1012 ) Loss= 0.23035036\n",
      "1013 ) Loss= 0.3211541\n",
      "1014 ) Loss= 0.3388881\n",
      "1015 ) Loss= 0.27499482\n",
      "1016 ) Loss= 0.43748385\n",
      "1017 ) Loss= 0.26629713\n",
      "1018 ) Loss= 0.7837365\n",
      "1019 ) Loss= 0.2792864\n",
      "1020 ) Loss= 0.2293125\n",
      "1021 ) Loss= 0.20896809\n",
      "1022 ) Loss= 0.30655423\n",
      "1023 ) Loss= 0.2245962\n",
      "1024 ) Loss= 0.46579736\n",
      "1025 ) Loss= 0.25423732\n",
      "1026 ) Loss= 0.39646596\n",
      "1027 ) Loss= 0.26851898\n",
      "1028 ) Loss= 0.38156202\n",
      "1029 ) Loss= 0.34803033\n",
      "1030 ) Loss= 0.4348288\n",
      "1031 ) Loss= 0.32558143\n",
      "1032 ) Loss= 0.30748317\n",
      "1033 ) Loss= 0.25201854\n",
      "1034 ) Loss= 0.39336392\n",
      "1035 ) Loss= 0.36033082\n",
      "1036 ) Loss= 0.24076724\n",
      "1037 ) Loss= 0.24520127\n",
      "1038 ) Loss= 0.51430076\n",
      "1039 ) Loss= 0.20937873\n",
      "1040 ) Loss= 0.5044251\n",
      "1041 ) Loss= 0.2736723\n",
      "1042 ) Loss= 0.40716562\n",
      "1043 ) Loss= 0.4027705\n",
      "1044 ) Loss= 0.35630852\n",
      "1045 ) Loss= 0.37163875\n",
      "1046 ) Loss= 0.26414567\n",
      "1047 ) Loss= 0.41281125\n",
      "1048 ) Loss= 0.32318988\n",
      "1049 ) Loss= 0.49552128\n",
      "1050 ) Loss= 0.21485175\n",
      "1051 ) Loss= 0.23470598\n",
      "1052 ) Loss= 0.6904723\n",
      "1053 ) Loss= 0.30337083\n",
      "1054 ) Loss= 0.2889717\n",
      "1055 ) Loss= 0.5718504\n",
      "1056 ) Loss= 0.25686905\n",
      "1057 ) Loss= 0.35293812\n",
      "1058 ) Loss= 0.21577923\n",
      "1059 ) Loss= 0.41010407\n",
      "1060 ) Loss= 0.35827217\n",
      "1061 ) Loss= 0.20796509\n",
      "1062 ) Loss= 0.2656418\n",
      "1063 ) Loss= 0.3218338\n",
      "1064 ) Loss= 0.39887187\n",
      "1065 ) Loss= 0.6041027\n",
      "1066 ) Loss= 0.3727706\n",
      "1067 ) Loss= 0.5297949\n",
      "1068 ) Loss= 0.4192009\n",
      "1069 ) Loss= 0.39343286\n",
      "1070 ) Loss= 0.24235113\n",
      "1071 ) Loss= 0.2011142\n",
      "1072 ) Loss= 0.31542444\n",
      "1073 ) Loss= 0.2900569\n",
      "1074 ) Loss= 0.3297678\n",
      "1075 ) Loss= 0.2427442\n",
      "1076 ) Loss= 0.20605683\n",
      "1077 ) Loss= 0.33687124\n",
      "1078 ) Loss= 0.21557163\n",
      "1079 ) Loss= 0.49784547\n",
      "1080 ) Loss= 0.32232344\n",
      "1081 ) Loss= 0.24949971\n",
      "1082 ) Loss= 0.23776619\n",
      "1083 ) Loss= 0.27449134\n",
      "1084 ) Loss= 0.21957022\n",
      "1085 ) Loss= 0.3649181\n",
      "1086 ) Loss= 0.26817697\n",
      "1087 ) Loss= 0.29685873\n",
      "1088 ) Loss= 0.23866533\n",
      "1089 ) Loss= 0.36092392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32069/1834381767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m    \u001b[0mLoss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Backpropogate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m    \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Apply gradient descent change to weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m    \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get  prediction classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\") Loss=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hypertrophy/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hypertrophy/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hypertrophy/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fused'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                  found_inf=found_inf)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hypertrophy/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m          found_inf=found_inf)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hypertrophy/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#----------------Train--------------------------------------------------------------------------\n",
    "for itr in range(10000): # Training loop\n",
    "   images,ann=LoadBatch() # Load taining batch\n",
    "   images=torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n",
    "   ann = torch.autograd.Variable(ann, requires_grad=False).to(device) # Load annotation\n",
    "   Pred=Net(images)['out'] # make prediction\n",
    "   Net.zero_grad()\n",
    "   criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
    "   Loss=criterion(Pred,ann.long()) # Calculate cross entropy loss\n",
    "   Loss.backward() # Backpropogate loss\n",
    "   optimizer.step() # Apply gradient descent change to weight\n",
    "   seg = torch.argmax(Pred[0], 0).cpu().detach().numpy()  # Get  prediction classes\n",
    "   print(itr,\") Loss=\",Loss.data.cpu().numpy())\n",
    "   \n",
    "   if itr % 1000 == 0: #Save model weight once every 60k steps permenant file\n",
    "        print(\"Saving Model\" +str(itr) + \".torch\")\n",
    "        torch.save(Net.state_dict(),   str(itr) + \".torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypertrophy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16 (default, Jan 17 2023, 22:20:44) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fab640fafe63b1fa1ad89386334ad1f3bfbfe350729bb4fe9539462594b39904"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
